{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Third protein mapping implementation\n",
    "\n",
    "Main idea:\n",
    "- get from CATH domain_ids + cath_domain_coordinates\n",
    "- get from RSCB protein_seq\n",
    "- search in RSCB for  rscb_domain_coordinates\n",
    "  - exact match of domain_id\n",
    "  - cath_domain_coordinates\n",
    "  - length\n",
    "  - seq alignment\n",
    "- cut domain_seq from protein using rscb_domain_coordinates\n",
    "\n",
    "Problem:\n",
    "- still some of domain_ids could not be mapped\n",
    "\n",
    "TODO:\n",
    "- сheck seq alignment\n",
    "- ~~automate remaining exceptions~~ make research if it even makes sense"
   ],
   "id": "bd3862ac2e6d6237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T20:17:41.115387Z",
     "start_time": "2025-06-01T20:17:40.051144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/subset.csv\")\n",
    "required_columns = [\"domain_parts\", \"length\", \"cath_domain_start1\", \"cath_domain_end1\"]\n",
    "total_rows = len(df)\n",
    "\n",
    "incomplete_rows_df = df[df[required_columns].isnull().any(axis=1)]\n",
    "incomplete_rows = len(incomplete_rows_df)\n",
    "\n",
    "print(f\"{incomplete_rows} rows with missing required fields out of {total_rows} total rows\")\n",
    "\n",
    "domain_column = 'domain_id'\n",
    "incomplete_ids = set(incomplete_rows_df[domain_column].dropna().tolist())\n",
    "\n",
    "fasta_path = \"../data/cath-domain-seqs.fa\"\n",
    "results = {}\n",
    "\n",
    "with open(fasta_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            header = line.strip()\n",
    "            last_part = header.split('|')[-1]\n",
    "            if '/' in last_part:\n",
    "                domain_id, region = last_part.split('/')\n",
    "                if domain_id in incomplete_ids:\n",
    "                    results[domain_id] = region\n",
    "\n",
    "\n",
    "for domain_id, region in results.items():\n",
    "    print(f\"{domain_id}: {region}\")"
   ],
   "id": "e624f106f876a517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows with missing required fields out of 11774 total rows\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "New problem:\n",
    "- more than 1.5k domains have multiple sites\n",
    "- now they all are parsed\n",
    "\n",
    "TODO:\n",
    "- make some research\n",
    "- find algorithm to work with them"
   ],
   "id": "aa32d1efcc2e1130"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T00:39:11.577002Z",
     "start_time": "2025-05-31T23:03:59.319296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#to be done\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from Bio.Align import PairwiseAligner\n",
    "\n",
    "checkpoint_dir = \"../data/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"mapped_checkpoint_*.csv\"))\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    df = pd.read_csv(latest_checkpoint)\n",
    "    start_index = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])\n",
    "    print(f\"[Resume] Loaded checkpoint at row {start_index}\")\n",
    "else:\n",
    "    df = pd.read_csv(\"../data/subset.csv\")\n",
    "    df[\"protein_sequence\"] = \"\"\n",
    "    df[\"domain_start\"] = None\n",
    "    df[\"domain_end\"] = None\n",
    "    df[\"domain_sequence\"] = \"\"\n",
    "    start_index = 0\n",
    "    print(\"[Start] No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "cath_fasta = {\n",
    "    record.id.split(\"|\")[-1].split(\"/\")[0]: record\n",
    "    for record in SeqIO.parse(\"../data/cath-domain-seqs.fa\", \"fasta\")\n",
    "}\n",
    "\n",
    "pdb_overrides = {\n",
    "    \"1vw4\": {\"pdb\": \"3j6b\", \"chain\": \"8\"},\n",
    "    \"4gns\": {\"pdb\": \"4yg8\", \"chain\": \"A\"},\n",
    "    \"1vs9\": {\"pdb\": \"4v4i\", \"chain\": \"S\"},\n",
    "    \"3p9d\": {\"pdb\": \"4v81\", \"chain\": \"A\"},\n",
    "    \"4d8q\": {\"pdb\": \"4v94\", \"chain\": \"F\"},\n",
    "    \"4a17\": {\"pdb\": \"4v8p\", \"chain\": \"BE\"},\n",
    "}\n",
    "\n",
    "def get_fasta_sequence_with_label(pdb_id, auth_chain_id):\n",
    "    url = f\"https://www.rcsb.org/fasta/entry/{pdb_id}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return \"\", None\n",
    "\n",
    "        fasta_blocks = response.text.strip().split(\">\")\n",
    "        for block in fasta_blocks:\n",
    "            lines = block.strip().splitlines()\n",
    "            if not lines:\n",
    "                continue\n",
    "            header = lines[0]\n",
    "            sequence = \"\".join(lines[1:])\n",
    "\n",
    "            if \"|Chain \" in header or \"|Chains \" in header:\n",
    "                chain_field = header.split(\"|\")[1]\n",
    "                parts = chain_field.replace(\"Chains \", \"\").replace(\"Chain \", \"\").split(\",\")\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if \"[auth \" in part:\n",
    "                        model_id = part.split(\"[auth\")[0].strip()\n",
    "                        auth_id = part.split(\"[auth\")[1].replace(\"]\", \"\").strip()\n",
    "                        if auth_chain_id == auth_id:\n",
    "                            return sequence, model_id\n",
    "                    else:\n",
    "                        if auth_chain_id == part:\n",
    "                            return sequence, auth_chain_id\n",
    "        return \"\", None\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to fetch FASTA for PDB ID {pdb_id}, chain {auth_chain_id}: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def get_cath_coordinates(pdb_id, chain_id, domain_id, expected_length, domain_start=None, domain_end=None, reference_seq=None, protein_seq=None):\n",
    "    url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity_instance/{pdb_id}/{chain_id}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        features = data.get(\"rcsb_polymer_instance_feature\", [])\n",
    "        matched_coords = []\n",
    "\n",
    "        # Step 1: match by domain_id\n",
    "        for feature in features:\n",
    "            if feature.get(\"type\") != \"CATH\":\n",
    "                continue\n",
    "            for prop in feature.get(\"additional_properties\", []):\n",
    "                if prop.get(\"name\") == \"CATH_DOMAIN_ID\":\n",
    "                    ids = [v.strip().lower() for v in prop.get(\"values\", [])]\n",
    "                    if domain_id.strip().lower() in ids:\n",
    "                        coords = feature.get(\"feature_positions\", [])\n",
    "                        matched_coords.extend([(c[\"beg_seq_id\"], c[\"end_seq_id\"]) for c in coords])\n",
    "\n",
    "        if matched_coords:\n",
    "            return matched_coords\n",
    "\n",
    "        # Step 2: match by domain start/end\n",
    "        if domain_start and domain_end:\n",
    "            for feature in features:\n",
    "                if feature.get(\"type\") != \"CATH\":\n",
    "                    continue\n",
    "                coords = feature.get(\"feature_positions\", [])\n",
    "                for c in coords:\n",
    "                    if abs(c[\"beg_seq_id\"] - domain_start) <= 2 and abs(c[\"end_seq_id\"] - domain_end) <= 2:\n",
    "                        matched_coords.append((c[\"beg_seq_id\"], c[\"end_seq_id\"]))\n",
    "            if matched_coords:\n",
    "                return matched_coords\n",
    "\n",
    "        # Step 3: match by domain length\n",
    "        for feature in features:\n",
    "            if feature.get(\"type\") != \"CATH\":\n",
    "                continue\n",
    "            coords = feature.get(\"feature_positions\", [])\n",
    "            for c in coords:\n",
    "                length = c[\"end_seq_id\"] - c[\"beg_seq_id\"] + 1\n",
    "                if expected_length - 2 <= length <= expected_length + 2:\n",
    "                    matched_coords.append((c[\"beg_seq_id\"], c[\"end_seq_id\"]))\n",
    "        if matched_coords:\n",
    "            return matched_coords\n",
    "\n",
    "        # Step 4: global sequence alignment\n",
    "        if reference_seq and protein_seq:\n",
    "            aligner = PairwiseAligner()\n",
    "            best_score = -1\n",
    "            best_start = None\n",
    "            for i in range(0, len(protein_seq) - expected_length + 1):\n",
    "                window = protein_seq[i:i+expected_length]\n",
    "                score = aligner.score(reference_seq[:expected_length], window)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_start = i\n",
    "            if best_start is not None:\n",
    "                return [(best_start + 1, best_start + expected_length)]\n",
    "\n",
    "        print(f\"[No Match] Could not map {domain_id}\")\n",
    "        return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to fetch CATH data for {pdb_id} {chain_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if idx < start_index:\n",
    "        continue\n",
    "\n",
    "    domain_id = row.get(\"domain_id\", \"\")\n",
    "    if not isinstance(domain_id, str) or len(domain_id) < 6:\n",
    "        continue\n",
    "\n",
    "    base_id = domain_id[:4].lower()\n",
    "    if base_id in pdb_overrides:\n",
    "        pdb_id = pdb_overrides[base_id][\"pdb\"]\n",
    "        auth_chain_id = pdb_overrides[base_id][\"chain\"]\n",
    "    else:\n",
    "        pdb_id = base_id\n",
    "        auth_chain_id = domain_id[4]\n",
    "\n",
    "    cath_seq = str(row.get(\"sequence\", \"\")).strip()\n",
    "    domain_start = row.get(\"cath_domain_start\")\n",
    "    domain_end = row.get(\"cath_domain_end\")\n",
    "    expected_length = int(row.get(\"length\", 0))\n",
    "\n",
    "    ref_seq_record = cath_fasta.get(domain_id)\n",
    "    reference_seq = str(ref_seq_record.seq) if ref_seq_record else None\n",
    "\n",
    "    protein_seq, label_asym_id = get_fasta_sequence_with_label(pdb_id, auth_chain_id)\n",
    "    if not protein_seq or not label_asym_id:\n",
    "        continue\n",
    "\n",
    "    df.at[idx, \"protein_sequence\"] = protein_seq\n",
    "\n",
    "    coordinates = get_cath_coordinates(\n",
    "        pdb_id, label_asym_id, domain_id, expected_length,\n",
    "        domain_start, domain_end, reference_seq, protein_seq\n",
    "    )\n",
    "\n",
    "    if not coordinates:\n",
    "        continue\n",
    "\n",
    "    full_seq = \"\"\n",
    "    starts, ends = [], []\n",
    "\n",
    "    for beg, end in coordinates:\n",
    "        starts.append(beg)\n",
    "        ends.append(end)\n",
    "        full_seq += protein_seq[beg - 1:end]\n",
    "\n",
    "    df.at[idx, \"domain_start\"] = min(starts)\n",
    "    df.at[idx, \"domain_end\"] = max(ends)\n",
    "    df.at[idx, \"domain_sequence\"] = full_seq\n",
    "\n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        df.to_csv(\"../data/subset_protein_mapped.csv\", index=False)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"mapped_checkpoint_{idx + 1}.csv\")\n",
    "        df.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"[Checkpoint] Saved progress at row {idx + 1} → {checkpoint_path}\")\n",
    "\n",
    "df.to_csv(\"../data/subset_protein_mapped.csv\", index=False)\n",
    "print(\"[Final] Saved full dataframe\")"
   ],
   "id": "245326e7f76f7527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Resume] Loaded checkpoint at row 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 5000/10264 [14:12<1:25:36,  1.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 5000 → ../data/checkpoints/mapped_checkpoint_5000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 5079/10264 [15:20<1:10:41,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[No Match] Could not map 8fm5A01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6000/10264 [29:19<1:22:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 6000 → ../data/checkpoints/mapped_checkpoint_6000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 7000/10264 [45:16<57:00,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 7000 → ../data/checkpoints/mapped_checkpoint_7000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 8000/10264 [1:01:33<40:36,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 8000 → ../data/checkpoints/mapped_checkpoint_8000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 9000/10264 [1:17:05<21:48,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 9000 → ../data/checkpoints/mapped_checkpoint_9000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 10000/10264 [1:31:18<04:37,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint] Saved progress at row 10000 → ../data/checkpoints/mapped_checkpoint_10000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10264/10264 [1:35:04<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final] Saved full dataframe\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T00:39:21.404794Z",
     "start_time": "2025-06-01T00:39:20.913094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/subset_protein_mapped.csv\")\n",
    "df.replace('<null>', np.nan, inplace=True)\n",
    "incomplete_rows = df[df.isnull().any(axis=1)]\n",
    "total_rows = len(df)\n",
    "incomplete_count = len(incomplete_rows)\n",
    "\n",
    "print(\"Domain IDs with missing values:\")\n",
    "print(incomplete_rows['domain_id'].tolist())\n",
    "print(f\"\\n{incomplete_count} rows with missing values out of {total_rows} total rows\")\n",
    "\n",
    "df = df.dropna()\n",
    "df.to_csv(\"../data/subset_protein_mapped.csv\", index=False)"
   ],
   "id": "47418ac115bef0d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain IDs with missing values:\n",
      "['1vx4K02', '4hubI02', '4u2vA02', '7cosB02', '1mslA02', '3s6n201', '2j01501', '3u5cE04', '2v49501', '8h9f601', '2a01C01', '4kbqD00', '7zo9A01', '1v1hB00', '3rjrB02', '3o58g00', '3o58d00', '1vwxA02', '3so1E00', '8fm5A01', '1vzrA01', '4i4m600', '4a18X01', '4p6vF01', '2m25A00', '1vx4407', '3kitJ00', '1vx7200', '1vwxr00', '4a18O00', '1vx2P00', '3o30I00', '3u5eJ00', '4k0mC02', '3o58A02', '7yyqA01', '8edjA01', '3o58D00', '1vw3D00', '1vwxP00', '1vx7W00', '3o58J00', '3o58Y02', '3kypE01']\n",
      "\n",
      "44 rows with missing values out of 10264 total rows\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
