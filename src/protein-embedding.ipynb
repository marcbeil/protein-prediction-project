{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Protein per-residue Embeddings\n",
    "- Compute all protein per-residue embeddings using Prot-T5\n",
    "## Input\n",
    "- list of proteins containing the following fields:\n",
    "-\n",
    "pollys_output = ('class', 'architecture', 'topology', 'homology', 'domain_id', 's35',\n",
    "'s60', 's95', 's100', 's100_count', 'length', 'resolution', 'domain_sequence',\n",
    "'homology_path', 'protein_sequence', \"protein_id\", \"domain_start\", \"domain_end\")"
   ],
   "id": "11dff8890a830773"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T17:30:09.906588Z",
     "start_time": "2025-05-18T17:30:09.565219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"../data/example_protein_subset.csv\")\n",
    "model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "\n",
    "output_dir = \"../data/embeddings\""
   ],
   "id": "1418702e94af66e3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T17:30:14.809658Z",
     "start_time": "2025-05-18T17:30:09.954393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "print(f\"Loading model: {model_name}\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5EncoderModel.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"../data/example_protein_subset.csv\")\n",
    "print(f\"Dataset loaded with {len(dataset)} proteins\")\n",
    "print(f\"Columns: {dataset.columns}\")\n"
   ],
   "id": "fbc7690c496bb719",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model: Rostlab/prot_t5_xl_half_uniref50-enc\n",
      "Model loaded successfully\n",
      "Dataset loaded with 1 proteins\n",
      "Columns: Index(['class', 'architecture', 'topology', 'homology', 'domain_id', 's35',\n",
      "       's60', 's95', 's100', 's100_count', 'length', 'resolution', 'sequence',\n",
      "       'homology_path', 'protein_id', 'protein_sequence', 'domain_start',\n",
      "       'domain_end'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T17:30:15.750128Z",
     "start_time": "2025-05-18T17:30:14.817543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/proteins\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/domains\", exist_ok=True)\n",
    "\n",
    "# Dictionary to store embeddings\n",
    "all_protein_embeddings = {}\n",
    "all_domain_embeddings = {}\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 1  # Process one protein at a time due to varying sequence lengths\n",
    "\n",
    "# Process each sequence with error handling\n",
    "start_time = time.time()\n",
    "for index, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Processing proteins\"):\n",
    "    try:\n",
    "        # Get sequence and preprocess\n",
    "        sequence = row[\"protein_sequence\"]\n",
    "        domain_id = row[\"domain_id\"]\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        domain_start = row[\"domain_start\"]\n",
    "        domain_end = row[\"domain_end\"]\n",
    "\n",
    "        # Replace non-standard amino acids with X\n",
    "        sequence = sequence.replace('U', 'X').replace('Z', 'X').replace('O', 'X')\n",
    "\n",
    "        # Tokenize sequence\n",
    "        ids = tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = ids['input_ids'].to(device)\n",
    "        attention_mask = ids['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get per-residue embeddings (remove special tokens)\n",
    "        # The first token is a special start token, and we need to remove it\n",
    "        # We also need to remove padding tokens using the attention mask\n",
    "        per_residue_embeddings = embedding.last_hidden_state.squeeze()[0, :].cpu()\n",
    "        # Ensure we only keep embeddings for actual residues (not padding)\n",
    "        seq_len = len(sequence)\n",
    "        per_residue_embeddings = per_residue_embeddings[:seq_len]\n",
    "        domain_embeddings = per_residue_embeddings[domain_start - 1:domain_end]\n",
    "        # Store in dictionary with domain_id as key\n",
    "        all_protein_embeddings[protein_id] = per_residue_embeddings\n",
    "        all_domain_embeddings[domain_id] = domain_embeddings\n",
    "\n",
    "        # Save individual protein embedding to file\n",
    "        torch.save(per_residue_embeddings, f\"{output_dir}/proteins/{protein_id}.pt\")\n",
    "        torch.save(domain_embeddings, f\"{output_dir}/domains/{protein_id}.pt\")\n",
    "\n",
    "        if index % 10 == 0 and index > 0:\n",
    "            print(f\"Processed {index} proteins. Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing protein {index} ({row.get('domain_id', 'unknown')}): {str(e)}\")\n",
    "\n",
    "# Save all embeddings to one file\n",
    "torch.save(all_protein_embeddings, f\"{output_dir}/proteins/all_protein_embeddings.pt\")\n",
    "torch.save(all_protein_embeddings, f\"{output_dir}/domains/all_domain_embeddings.pt\")\n",
    "print(f\"All embeddings saved\")\n",
    "print(f\"Total time: {time.time() - start_time:.2f}s\")"
   ],
   "id": "b91da19379dad4cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings saved\n",
      "Total time: 0.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T17:30:15.773614Z",
     "start_time": "2025-05-18T17:30:15.772232Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b3c0d233d3107487",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
